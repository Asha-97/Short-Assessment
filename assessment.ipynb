{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "212b3c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing dataset\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import hashlib\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "df = pd.read_csv('iris.csv',sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e267c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "10f8b92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length  sepal_width  petal_length  petal_width  species\n",
       "False         False        False         False        False      150\n",
       "dtype: int64"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc50649d",
   "metadata": {},
   "source": [
    "# ! pip3 install jmespath\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "ad38a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_details(col, action):\n",
    "    for l in data['design_state_data'][i][j].keys():\n",
    "        #print(l)#//print\n",
    "        if l==\"feature_details\":\n",
    "            for m in data['design_state_data'][i][j][l].keys():\n",
    "                #print(data['design_state_data'][i][j][l][m])\n",
    "                if data['design_state_data'][i][j][l][m]==\"Impute\":\n",
    "                    if (df.columns[0]==col) or (df.columns[2]==col):\n",
    "                        df[j] = df[j].fillna(action)\n",
    "                        #print('finished')\n",
    "                    elif (df.columns[1]==col) or (df.columns[3]==col):\n",
    "                        df[j]=df[j].fillna(data['design_state_data'][i][j][l][\"impute_value\"])\n",
    "                        #print('finished')\n",
    "                elif df.columns[4]==col:\n",
    "                    for s in df['species']:\n",
    "                        #print(s)\n",
    "                        df['species']=hash(s)\n",
    "                        break    \n",
    "    \n",
    "                #my_dict.values()).index(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "30733237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_details(selected_algo):\n",
    "    #print(\"algo details\", selected_algo)\n",
    "    reg=\"\"\n",
    "   # for k in data['design_state_data'][i][j].keys():\n",
    "    for k in selected_algo.keys():\n",
    "        temp = iter(selected_algo)\n",
    "        if selected_algo[k]==\"Random Forest Regressor\":  \n",
    "            #print(\"model_name\",selected_algo[k])\n",
    "            reg=RandomForestRegressor(max_depth=selected_algo['max_depth'], \n",
    "                                      min_samples_leaf=selected_algo['min_samples_per_leaf_min_value'])\n",
    "            #print(\"Reg\", reg)\n",
    "        elif selected_algo[k]==\"Gradient Boosted Trees\":\n",
    "            reg=GradientBoostingRegressor(subsample=selected_algo['min_subsample'], max_depth=selected_algo['max_depth'], \n",
    "                                          n_iter_no_change=selected_algo['min_iter'])\n",
    "        elif selected_algo[k]==\"LinearRegression\":\n",
    "            reg=LinearRegression(fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
    "        elif selected_algo[k]==\"LogisticRegression\":\n",
    "            reg=LogisticRegression(max_iter=selected_algo['max_iter'], C=100, multi_class='ovr')\n",
    "        elif selected_algo[k]==\"RidgeRegression\":\n",
    "            reg=Ridge(max_iter=selected_algo['max_iter'])\n",
    "        elif selected_algo[k]==\"Lasso Regression\":\n",
    "            reg=Lasso(max_iter=selected_algo['max_iter'])\n",
    "        elif selected_algo[k]==\"Decision Tree\":\n",
    "            reg=DecisionTreeRegressor(max_depth=selected_algo['max_depth'])\n",
    "    #print('Reg: ',reg)\n",
    "    return reg\n",
    "\n",
    "\n",
    "def train(algorithm):\n",
    "    #print(df)\n",
    "    X=df.drop(target_col_name, axis=1)\n",
    "    #print(X)\n",
    "    match = re.search(r'Logistic', str(algorithm))\n",
    "    if match:\n",
    "        y=df[target_col_name].astype('int')\n",
    "        X=df.astype('int')\n",
    "        sc=StandardScaler()\n",
    "        sc.fit(X)\n",
    "        X=sc.transform(X)\n",
    "       # y=sc.transform(y).reshape(1, -1)\n",
    "        #print(X)\n",
    "        #print(y)\n",
    "    else:\n",
    "        y=df[target_col_name]\n",
    "    #print(\"reg: \", algorithm)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def run_predict(algorithm):\n",
    "    X, y=train(algorithm)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    algorithm.fit(X_train, y_train)\n",
    "    y_pred=algorithm.predict(X_test)\n",
    "    print(\"i) Predicted values \")\n",
    "    print(y_pred)\n",
    "    \n",
    "    \n",
    "    print(\"i) Before Hypertuning\")\n",
    "    metrics(y_test, y_pred)\n",
    "    \n",
    "\n",
    "def metrics(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_true=y_test,y_pred=y_pred)\n",
    "    #squared True returns MSE value, False returns RMSE value.\n",
    "    mse = mean_squared_error(y_true=y_test,y_pred=y_pred) #default=True\n",
    "    rmse = mean_squared_error(y_true=y_test,y_pred=y_pred,squared=False)\n",
    "    print(\"MAE:\",mae)\n",
    "    print(\"MSE:\",mse)\n",
    "    print(\"RMSE:\",rmse)\n",
    "\n",
    "def hyperparametering_GVgridSearch(algorithm, pos):\n",
    "    # Define the grid \n",
    "    param={\n",
    "    }\n",
    "    param_grid = pos\n",
    "    for a in param_grid: \n",
    "        #print(\"key: \", a, \" val: \", [param_grid[a]])\n",
    "        if a in list(algorithm.get_params().keys()):\n",
    "            param[a]=[param_grid[a]]\n",
    "            #param[a]=[param_grid[a]]\n",
    "    \n",
    "    #print(\"param_grid:\", param)\n",
    "        # Instantiate GridSearchCV\n",
    "    model_gridsearch = GridSearchCV(\n",
    "    estimator=algorithm, param_grid=param, scoring='neg_mean_squared_error', n_jobs=4, cv=5,\n",
    "    refit=True, return_train_score=True)\n",
    "    X, y=train(algorithm)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    #print(algorithm.get_params().keys())\n",
    "    # Fit the selected model\n",
    "    model_gridsearch.fit(X_train, y_train)\n",
    "    # Print the time spend and number of models ran\n",
    "    #print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\" % ((time() - start), len(model_gridsearch.cv_results_['params'])))\n",
    "    y_pred_grid = model_gridsearch.predict(X_test)\n",
    "    print(\"ii) Predicted values using GridSearchCV\")\n",
    "    print(y_pred_grid)\n",
    "    \n",
    "    \n",
    "    print(\"After Hypertuning using GridSearchCV\")\n",
    "    metrics(y_test, y_pred_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f6edd37b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Reading the Target variable and Type of regression to be done!!! \n",
      "i)\n",
      "Target variable:  petal_width\n",
      "value:  0      0.2\n",
      "1      0.2\n",
      "2      0.2\n",
      "3      0.2\n",
      "4      0.2\n",
      "      ... \n",
      "145    2.3\n",
      "146    1.9\n",
      "147    2.0\n",
      "148    2.3\n",
      "149    1.8\n",
      "Name: petal_width, Length: 150, dtype: float64\n",
      "iI)\n",
      "petal_width of type :  regression\n",
      "2) Read the features (which are column names in the csv) and figure out what missing imputation needs to be applied and apply that to the columns loaded in a dataframe\n",
      "i) \n",
      "Before handling feature :\n",
      "     sepal_length  sepal_width  petal_length  petal_width         species\n",
      "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
      "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
      "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
      "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
      "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
      "..            ...          ...           ...          ...             ...\n",
      "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
      "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
      "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
      "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
      "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "ii) \n",
      "After handling feature :\n",
      "     sepal_length  sepal_width  petal_length  petal_width              species\n",
      "0             5.1          3.5           1.4          0.2  2295829119547317674\n",
      "1             4.9          3.0           1.4          0.2  2295829119547317674\n",
      "2             4.7          3.2           1.3          0.2  2295829119547317674\n",
      "3             4.6          3.1           1.5          0.2  2295829119547317674\n",
      "4             5.0          3.6           1.4          0.2  2295829119547317674\n",
      "..            ...          ...           ...          ...                  ...\n",
      "145           6.7          3.0           5.2          2.3  2295829119547317674\n",
      "146           6.3          2.5           5.0          1.9  2295829119547317674\n",
      "147           6.5          3.0           5.2          2.0  2295829119547317674\n",
      "148           6.2          3.4           5.4          2.3  2295829119547317674\n",
      "149           5.9          3.0           5.1          1.8  2295829119547317674\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "3) Compute feature reduction based on input. See the screenshot below where there can be No Reduction, Corr with Target, Tree-based, PCA.\n",
      "i) \n",
      "Before reducing feature :\n",
      "     sepal_length  sepal_width  petal_length  petal_width              species\n",
      "0             5.1          3.5           1.4          0.2  2295829119547317674\n",
      "1             4.9          3.0           1.4          0.2  2295829119547317674\n",
      "2             4.7          3.2           1.3          0.2  2295829119547317674\n",
      "3             4.6          3.1           1.5          0.2  2295829119547317674\n",
      "4             5.0          3.6           1.4          0.2  2295829119547317674\n",
      "..            ...          ...           ...          ...                  ...\n",
      "145           6.7          3.0           5.2          2.3  2295829119547317674\n",
      "146           6.3          2.5           5.0          1.9  2295829119547317674\n",
      "147           6.5          3.0           5.2          2.0  2295829119547317674\n",
      "148           6.2          3.4           5.4          2.3  2295829119547317674\n",
      "149           5.9          3.0           5.1          1.8  2295829119547317674\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "ii) After reducing feature :\n",
      "     sepal_length  sepal_width  petal_length  petal_width              species\n",
      "0             5.1          3.5           1.4          0.2  2295829119547317674\n",
      "1             4.9          3.0           1.4          0.2  2295829119547317674\n",
      "2             4.7          3.2           1.3          0.2  2295829119547317674\n",
      "3             4.6          3.1           1.5          0.2  2295829119547317674\n",
      "4             5.0          3.6           1.4          0.2  2295829119547317674\n",
      "..            ...          ...           ...          ...                  ...\n",
      "145           6.7          3.0           5.2          2.3  2295829119547317674\n",
      "146           6.3          2.5           5.0          1.9  2295829119547317674\n",
      "147           6.5          3.0           5.2          2.0  2295829119547317674\n",
      "148           6.2          3.4           5.4          2.3  2295829119547317674\n",
      "149           5.9          3.0           5.1          1.8  2295829119547317674\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "4) Parse the Json and make the model objects (using sklean) that can handle what is required in the “prediction_type” specified in the JSON (See #1 where “prediction_type” is specified). Keep in mind not to pick models that don’t apply for the prediction_type specified\n",
      "RandomForestClassifier  is Not a Regression Algorithm\n",
      "Selected regression Algorithm:  RandomForestRegressor\n",
      "5) Run the fit and predict on each model – keep in mind that you need to do hyper parameter tuning i.e., use GridSearchCV\n",
      "i) Predicted values \n",
      "[1.95978088 2.07053939 2.16092772 1.78695177 1.10723788 0.20826927\n",
      " 0.34879938 1.23738827 0.29808409 1.75132394 1.45157957 1.2840265\n",
      " 1.45346518 1.6615122  1.68253813 1.60658464 1.4603129  1.35433709\n",
      " 0.24693743 0.21099138 1.38751139 2.17475632 0.26090836 1.64993523\n",
      " 2.25789752 1.10877121 0.24693743 0.31387085 1.72203979 0.21252817\n",
      " 1.10780498 0.21517443 1.83051565 2.28481762 0.33729166 1.61946754\n",
      " 2.25906618 1.04292284]\n",
      "i) Before Hypertuning\n",
      "MAE: 0.14800377618778301\n",
      "MSE: 0.0392747680462734\n",
      "RMSE: 0.19817862661314767\n",
      "ii) Predicted values using GridSearchCV\n",
      "[0.18873594 2.11202926 0.18873594 1.92294566 1.77597577 1.98411312\n",
      " 1.89265873 1.4802113  0.22101573 0.19721757 2.11922451 1.8869596\n",
      " 1.40550476 2.1648307  1.07847161 1.83360715 1.4993497  1.70340203\n",
      " 0.19775328 1.20129689 0.32973854 1.3558094  1.35880808 1.33306501\n",
      " 0.19775328 0.2212318  1.66224233 0.26056665 1.9489726  2.05590871\n",
      " 0.25685175 0.31507844 0.18000411 1.27661568 1.46793305 1.70787124\n",
      " 0.25274308 2.06320734]\n",
      "After Hypertuning using GridSearchCV\n",
      "MAE: 0.14594612986828953\n",
      "MSE: 0.040715596278629994\n",
      "RMSE: 0.2017810602574731\n",
      "GBTClassifier  is Not a Regression Algorithm\n",
      "Selected regression Algorithm:  GBTRegressor\n",
      "5) Run the fit and predict on each model – keep in mind that you need to do hyper parameter tuning i.e., use GridSearchCV\n",
      "i) Predicted values \n",
      "[2.36083917 2.15382572 0.43139013 1.99545732 2.22537623 1.49088191\n",
      " 1.29440261 0.20447672 2.03311074 2.05285141 0.26118194 0.31058151\n",
      " 1.2945687  0.21573854 1.87629252 0.28348843 0.27072453 0.33617093\n",
      " 1.48545657 2.09574245 0.21573854 1.42212521 1.40906614 1.78885652\n",
      " 1.48545657 0.47031089 1.86586393 0.13879327 1.53936892 1.29440261\n",
      " 0.24042575 0.21916888 1.2903877  1.86567146 1.29893913 0.25096998\n",
      " 1.78481419 0.4306602 ]\n",
      "i) Before Hypertuning\n",
      "MAE: 0.17449437126090042\n",
      "MSE: 0.055082121464586\n",
      "RMSE: 0.2346958062356164\n",
      "ii) Predicted values using GridSearchCV\n",
      "[0.2004159  2.08560493 1.08851198 1.94255396 1.37850785 1.97801088\n",
      " 0.22861136 2.1842156  2.49972869 1.73941056 1.29307039 2.26299505\n",
      " 2.1761395  0.24623197 1.34803673 1.14599497 1.00002176 2.2406628\n",
      " 2.34584436 1.32270547 1.37824073 0.4001963  0.10016994 1.49942126\n",
      " 0.20009356 0.28242733 1.88388828 0.20061531 1.99105054 0.16076936\n",
      " 0.16077243 1.37552304 1.35623768 0.19987193 2.13439728 1.38662092\n",
      " 1.90009262 1.29326658]\n",
      "After Hypertuning using GridSearchCV\n",
      "MAE: 0.19113552221170488\n",
      "MSE: 0.06374050695706714\n",
      "RMSE: 0.25246882373288615\n",
      "Selected regression Algorithm:  LinearRegression\n",
      "5) Run the fit and predict on each model – keep in mind that you need to do hyper parameter tuning i.e., use GridSearchCV\n",
      "i) Predicted values \n",
      "[1.4248657  2.08230602 1.70103465 1.57237262 1.48389471 1.40583687\n",
      " 1.96735217 2.26474203 1.81874921 0.26068616 0.834748   1.87449011\n",
      " 1.56695805 0.23589928 1.43719998 0.29129671 1.55587935 1.79608879\n",
      " 1.32750116 0.24487984 0.2617822  1.64282381 0.16256907 0.34916407\n",
      " 0.16003352 1.19116687 1.73746886 1.84278353 0.277932   1.60476616\n",
      " 0.04279366 2.14058247 0.18735595 0.32935436 2.00609678 1.77621347\n",
      " 1.54724227 1.45328417]\n",
      "i) Before Hypertuning\n",
      "MAE: 0.13013082867035627\n",
      "MSE: 0.032055170045046266\n",
      "RMSE: 0.17903957675621965\n",
      "ii) Predicted values using GridSearchCV\n",
      "[0.31614258 1.35037956 1.98099942 1.98475794 0.98905545 1.41667291\n",
      " 0.27740451 1.31760936 2.01019662 0.31941926 1.7835758  0.17884087\n",
      " 1.61505304 1.43188531 0.0608064  1.95064214 1.33737651 2.11117319\n",
      " 1.36338262 1.72595956 1.20228414 0.22104112 1.64135548 2.02039767\n",
      " 1.82914548 1.02317125 1.6701636  0.34158127 1.34507188 1.30576639\n",
      " 1.26336256 0.12498315 1.51523656 1.69589861 1.98475794 1.79726426\n",
      " 1.74272113 0.20803807]\n",
      "After Hypertuning using GridSearchCV\n",
      "MAE: 0.1524795949843365\n",
      "MSE: 0.044312543875808345\n",
      "RMSE: 0.2105054485656092\n",
      "Selected regression Algorithm:  LogisticRegression\n",
      "5) Run the fit and predict on each model – keep in mind that you need to do hyper parameter tuning i.e., use GridSearchCV\n",
      "i) Predicted values \n",
      "[0 2 0 2 1 1 1 1 0 0 1 0 2 1 2 1 2 1 1 0 0 0 2 0 0 2 1 0 0 0 0 0 2 2 1 0 1\n",
      " 1]\n",
      "i) Before Hypertuning\n",
      "MAE: 0.0\n",
      "MSE: 0.0\n",
      "RMSE: 0.0\n",
      "ii) Predicted values using GridSearchCV\n",
      "[1 2 0 1 2 1 2 2 0 0 2 0 1 0 1 2 1 1 1 2 0 1 1 1 0 0 2 1 1 1 0 0 2 2 0 1 1\n",
      " 1]\n",
      "After Hypertuning using GridSearchCV\n",
      "MAE: 0.0\n",
      "MSE: 0.0\n",
      "RMSE: 0.0\n",
      "Selected regression Algorithm:  RidgeRegression\n",
      "5) Run the fit and predict on each model – keep in mind that you need to do hyper parameter tuning i.e., use GridSearchCV\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i) Predicted values \n",
      "[2.21781565 0.24458867 0.11910839 1.99316705 2.0019925  0.34876817\n",
      " 0.3136427  0.23880286 1.70602487 1.49111711 0.19333797 0.26390597\n",
      " 2.07575257 2.2917401  1.84778282 1.57948844 1.16580983 1.98375496\n",
      " 1.6115626  0.32291403 0.24458867 1.61355776 1.71590647 1.21933721\n",
      " 1.77477377 0.25416514 0.22618676 1.93933454 1.60337103 1.9727936\n",
      " 1.77324812 1.47318471 1.62435475 0.20503867 1.80214188 1.76489217\n",
      " 1.49812345 2.08488315]\n",
      "i) Before Hypertuning\n",
      "MAE: 0.1677142041431399\n",
      "MSE: 0.0421957358455294\n",
      "RMSE: 0.2054160067899515\n",
      "ii) Predicted values using GridSearchCV\n",
      "[1.20094699 0.16991827 0.99734295 2.07063412 0.37946082 1.51761723\n",
      " 1.39085206 1.09675848 1.8164355  0.15403313 0.07684191 1.77949159\n",
      " 0.99110038 1.23900814 0.19691555 2.03557232 1.34431388 2.15229427\n",
      " 0.03020713 1.76213686 1.91214697 1.60034631 1.98081286 0.25568312\n",
      " 1.61359634 1.12888111 0.21386963 0.32546624 0.25456589 0.8281926\n",
      " 2.33290913 1.99527671 0.40163682 1.47214799 1.2305311  1.57116286\n",
      " 1.81126186 0.24086692]\n",
      "After Hypertuning using GridSearchCV\n",
      "MAE: 0.12297140795831644\n",
      "MSE: 0.022232367981780493\n",
      "RMSE: 0.14910522452878872\n",
      "Selected regression Algorithm:  LassoRegression\n",
      "5) Run the fit and predict on each model – keep in mind that you need to do hyper parameter tuning i.e., use GridSearchCV\n",
      "i) Predicted values \n",
      "[1.04183841 1.03273532 1.22390031 1.02363223 1.26941578 1.32403435\n",
      " 1.36954982 1.03273532 1.04183841 1.0600446  1.25120959 1.03273532\n",
      " 1.0600446  1.44237458 1.36954982 1.04183841 1.20569412 1.35134363\n",
      " 1.03273532 1.4059622  1.4059622  1.48789005 1.05094151 1.33313744\n",
      " 1.04183841 1.03273532 1.17838483 1.33313744 1.27851887 1.38775601\n",
      " 1.02363223 1.04183841 1.46058077 1.41506529 1.33313744 1.41506529\n",
      " 1.43327148 1.05094151]\n",
      "i) Before Hypertuning\n",
      "MAE: 0.5007898919141456\n",
      "MSE: 0.3488067157872625\n",
      "RMSE: 0.5905986080133127\n",
      "ii) Predicted values using GridSearchCV\n",
      "[1.25447592 1.1729563  1.30635204 0.95804093 1.23965417 1.26188679\n",
      " 1.26188679 0.98027356 1.29894117 1.35822817 1.19518892 0.98768443\n",
      " 1.00250618 0.98768443 0.99509531 1.2322433  1.00250618 1.33599554\n",
      " 0.99509531 1.26929767 1.29153029 0.98027356 0.98768443 1.29153029\n",
      " 1.00991706 0.99509531 1.24706505 1.32858467 1.33599554 1.32117379\n",
      " 1.21001067 1.16554542 1.31376292 1.28411942 1.21742155 1.10625843\n",
      " 1.18036717 1.21001067]\n",
      "After Hypertuning using GridSearchCV\n",
      "MAE: 0.6298679400155045\n",
      "MSE: 0.4989323689879217\n",
      "RMSE: 0.7063514486344045\n",
      "Selected regression Algorithm:  ElasticNetRegression\n",
      "5) Run the fit and predict on each model – keep in mind that you need to do hyper parameter tuning i.e., use GridSearchCV\n",
      "i) Predicted values \n",
      "[1.42335712 1.13739117 0.96376898 1.30080029 1.00462126 1.16803038\n",
      " 1.3314395  1.2293088  1.31101336 0.95355592 1.35186564 1.3620787\n",
      " 1.25994801 1.37229177 0.97398205 0.99440819 1.3314395  1.25994801\n",
      " 1.31101336 0.97398205 0.99440819 1.40293098 1.34165257 0.97398205\n",
      " 0.99440819 1.30080029 1.45399633 1.0250474  1.35186564 0.98419512\n",
      " 1.3314395  1.40293098 0.99440819 1.27016108 1.18845652 1.32122643\n",
      " 1.31101336 0.97398205]\n",
      "i) Before Hypertuning\n",
      "MAE: 0.47883146910358176\n",
      "MSE: 0.3348649573303339\n",
      "RMSE: 0.5786751742820266\n",
      "ii) Predicted values using GridSearchCV\n",
      "[1.4434536  0.95410206 1.36733225 0.94322758 0.97585102 1.26946194\n",
      " 0.96497654 1.28033642 0.95410206 1.30208538 1.22596403 1.21508955\n",
      " 1.28033642 1.41083017 1.51957495 1.22596403 0.96497654 1.2912109\n",
      " 1.30208538 1.47607704 1.34558329 0.97585102 1.23683851 1.41083017\n",
      " 0.92147863 0.96497654 1.23683851 0.97585102 1.33470882 1.22596403\n",
      " 1.32383434 0.95410206 1.31295986 1.25858746 1.39995569 1.23683851\n",
      " 0.95410206 1.42170465]\n",
      "After Hypertuning using GridSearchCV\n",
      "MAE: 0.4452066042145348\n",
      "MSE: 0.26471881231290384\n",
      "RMSE: 0.5145083209365071\n",
      "xg_boost  is Not a Regression Algorithm\n",
      "Selected regression Algorithm:  DecisionTreeRegressor\n",
      "5) Run the fit and predict on each model – keep in mind that you need to do hyper parameter tuning i.e., use GridSearchCV\n",
      "i) Predicted values \n",
      "[1.6        0.5        2.03333333 1.26       2.4        2.03333333\n",
      " 1.5        1.15       1.5        0.26666667 0.2        1.1\n",
      " 1.95       0.26666667 0.1        1.775      1.6        2.03333333\n",
      " 2.2        2.03333333 0.13333333 0.2        1.95       0.1\n",
      " 0.1        2.03333333 0.26666667 0.26666667 1.5        1.5\n",
      " 0.35       1.8        1.6        1.3        1.         0.35\n",
      " 1.95       2.03333333]\n",
      "i) Before Hypertuning\n",
      "MAE: 0.15609649122807012\n",
      "MSE: 0.03641527777777776\n",
      "RMSE: 0.1908278747399807\n",
      "ii) Predicted values using GridSearchCV\n",
      "[2.03333333 1.42857143 2.03333333 1.1        1.         1.25\n",
      " 1.42857143 2.325      0.4        1.3        1.42857143 0.2\n",
      " 1.3        1.25       1.         1.75       1.4        2.\n",
      " 0.3        0.5        1.25       0.2        0.3        2.03333333\n",
      " 1.3        1.3        0.2        2.03333333 1.25       0.1\n",
      " 0.25       1.42857143 2.03333333 2.03333333 1.4        2.3\n",
      " 2.         2.03333333]\n",
      "After Hypertuning using GridSearchCV\n",
      "MAE: 0.15266290726817042\n",
      "MSE: 0.038997531029955824\n",
      "RMSE: 0.19747792542447834\n",
      "DecisionTreeClassifier  is Not a Regression Algorithm\n",
      "SVM  is Not a Regression Algorithm\n",
      "SGD  is Not a Regression Algorithm\n",
      "KNN  is Not a Regression Algorithm\n",
      "extra_random_trees  is Not a Regression Algorithm\n",
      "neural_network  is Not a Regression Algorithm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "run_algo=\"\"\n",
    "target_col_name=\"\"\n",
    "selected=\"No Reduction\"\n",
    "\n",
    "with open('algoparams_from_ui.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    #print(data.keys())\n",
    "    if 'design_state_data' in data:\n",
    "        #print(data['design_state_data'].keys())\n",
    "        for i in data['design_state_data'].keys():\n",
    "            if i==\"target\":\n",
    "                print(\"1) Reading the Target variable and Type of regression to be done!!! \")\n",
    "                for j in data['design_state_data'][i].keys():\n",
    "                    if j==\"target\":\n",
    "                        target_col_name=data['design_state_data'][i][j]\n",
    "                        y=df.loc[:, target_col_name]\n",
    "                        print('i)')\n",
    "                        print(\"Target variable: \",target_col_name)\n",
    "                        print(\"value: \", y)\n",
    "                        #y=df[value]\n",
    "                        #print(y)\n",
    "                    elif j==\"type\":\n",
    "                        value=data['design_state_data'][i][j]\n",
    "                        df_type=value\n",
    "                        print('iI)')\n",
    "                        print(target_col_name+\" of type : \",value)\n",
    "\n",
    "            elif i==\"feature_handling\":\n",
    "                print(\"2) Read the features (which are column names in the csv) and figure out what missing imputation needs to be applied and apply that to the columns loaded in a dataframe\")\n",
    "                print(\"i) \")\n",
    "                print(\"Before handling feature :\")\n",
    "                print(df)\n",
    "                for j in data['design_state_data'][i].keys():\n",
    "                    #print(j)\n",
    "                    if ((j==df.columns[0]) or (j==df.columns[2])):\n",
    "                        action=df[j].mean()\n",
    "                        feature_details(j, action)\n",
    "                    elif ((j==df.columns[1]) or (j==df.columns[3])):\n",
    "                        feature_details(j, 2)                        \n",
    "                    elif j==df.columns[4]:\n",
    "                        feature_details(j, 1)\n",
    "                print(\"ii) \")\n",
    "                print(\"After handling feature :\")\n",
    "                print(df)\n",
    "\n",
    "            elif i==\"feature_reduction\":\n",
    "                print(\"3) Compute feature reduction based on input. See the screenshot below where there can be No Reduction, Corr with Target, Tree-based, PCA.\")\n",
    "                print(\"i) \")\n",
    "                print(\"Before reducing feature :\")\n",
    "                print(df)\n",
    "                for j in data['design_state_data'][i].keys():\n",
    "                    if not type(data['design_state_data'][i][j])==str:\n",
    "                        #print(\"j: \",j,\"  selected:\",selected)\n",
    "                        if j==selected:\n",
    "                            for k in data['design_state_data'][i][j].keys():\n",
    "                                if data['design_state_data'][i][j][k] == True:\n",
    "                                    df=df.iloc[:, 0:5]\n",
    "                                elif data['design_state_data'][i][j][k] == False:\n",
    "                                    df=df.iloc[:,0:0]\n",
    "                print(\"ii) After reducing feature :\")\n",
    "                print(df)\n",
    "                            #print(data['design_state_data'][i][j].keys())\n",
    "                            #if data['design_state_data'][i][j].keys()==str:\n",
    "                           \n",
    "            elif i=='algorithms': \n",
    "                print(\"4) Parse the Json and make the model objects (using sklean) that can handle what is required in the “prediction_type” specified in the JSON (See #1 where “prediction_type” is specified). Keep in mind not to pick models that don’t apply for the prediction_type specified\")\n",
    "                for j in data['design_state_data'][i].keys():\n",
    "                    df_type=str(df_type)\n",
    "                    match = re.search(r'egress', j)\n",
    "                    #print(match)\n",
    "                    if match:\n",
    "                        #print(\"Found!\")\n",
    "                        #model building\n",
    "                        temp=data['design_state_data'][i][j]\n",
    "                        print('Selected regression Algorithm: ',j)\n",
    "                        run_algo=algorithm_details(temp)\n",
    "                        print(\"5) Run the fit and predict on each model – keep in mind that you need to do hyper parameter tuning i.e., use GridSearchCV\")\n",
    "                        run_predict(run_algo)\n",
    "                        position=data['design_state_data']['hyperparameters']\n",
    "                        hyperparametering_GVgridSearch(run_algo, position)\n",
    "                    else:\n",
    "                        print(j,' is Not a Regression Algorithm')\n",
    "               \n",
    "            '''\n",
    "            elif i=='hyperparameters':\n",
    "                for j in data['design_state_data'][i].keys():\n",
    "                    hyperparametering_GVgridSearch(run_algo)\n",
    "            \n",
    "            else:\n",
    "                break\n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f961b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
